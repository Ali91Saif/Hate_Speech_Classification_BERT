# -*- coding: utf-8 -*-
"""Hate_Speech_Classification_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bd_kHYNkpJY5GVwBXKwGHgqgJIX0q2Ed

**Installing Transformers**
"""

!pip install transformers

"""**Installing datasets**"""

!pip install datasets

# !pip install numpy==1.23.4

"""**Importing Libraries**"""

from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""**Mounting Google Drive**"""

from google.colab import drive
drive.mount('/content/drive')

"""**Importing and reading data and storing the DataFrame into variable 'df'**"""

df = pd.read_csv('/content/drive/MyDrive/labeled_data.csv')

df.head()

df.shape

"""### Here in this problem, we are interested only in two columns, namely [class] and [tweet]. So removing all other unnecessary columns:"""

df = df[['class','tweet']]
df.head()

"""### As this project is all about demonstration of the implementaion of fine tuning of pre-trained BERT model and it require multiple epochs to train, and every epochs will take larger amount of time to complete, so here i am going to consider only 3000 data instances to train this model, so that the training time will not take much longer."""

df = df[0:3001]
df.shape

df.head

df.info()

"""Checking for number of classes and their distributions in our dataset"""

df['class'].value_counts()

df['class'].value_counts(normalize = True) * 100

sns.countplot(x = 'class', data = df);

"""**Data Cleaning**

### We do not require much cleaning of data while working on BERT as it automatically takes care of these [stopwords, HTML syntax  etc]. It utilises them for their training and decision making.
### However, i will use regular experission for removing special characters.
"""

df['clean_tweet'] = df['tweet'].str.replace('@[A-Za-z0-9]+\s?', '', regex = True)

df.head()

"""**Importing Pandas Dataset**"""

from datasets import Dataset

ds = Dataset.from_pandas(df)
ds

"""**Splitting Data into train set, validation set, and test set**"""

from datasets import DatasetDict

train_test_valid = ds.train_test_split()
test_valid = train_test_valid['test'].train_test_split()

train_test_valid_dataset = DatasetDict({
    'train' : train_test_valid['train'],
    'test' : test_valid['test'],
    'valid' : test_valid['train']
})

dataset = train_test_valid_dataset
dataset

"""**Creating object for BERT Tokenizer**"""

tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')

"""**Tokenizing the texts from cleaned Tweets**"""

def token_fun(x):
  return tokenizer(x['clean_tweet'], padding = 'max_length', truncation = True)

tokenized_dataset = dataset.map(token_fun, batched = True)

tokenized_dataset

"""From the above Tokenized dataset, extracting Tokenized Training Dataset, Tokenized Testing Dataset, Tokenized Validation Dataset"""

train_dataset = tokenized_dataset['train']

eval_dataset = tokenized_dataset['valid']

test_dataset = tokenized_dataset['test']

train_dataset

"""Here after the Tokenization, tweet and clean_tweet varibles are no more required, so removing these columns from the data"""

train_set = train_dataset.remove_columns(['tweet', 'clean_tweet']).with_format('tensorflow')

tf_eval_dataset = eval_dataset.remove_columns(['tweet', 'clean_tweet']).with_format('tensorflow')

tf_test_dataset = test_dataset.remove_columns(['tweet', 'clean_tweet']).with_format('tensorflow')

train_features = { x: train_set[x] for x in tokenizer.model_input_names}
train_set_for_final_model = tf.data.Dataset.from_tensor_slices((train_features, train_set['class'] ))
train_set_for_final_model = train_set_for_final_model.shuffle(len(train_set)).batch(8)


eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}
val_set_for_final_model = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset["class"]))
val_set_for_final_model = val_set_for_final_model.batch(8)

test_features = {x: tf_test_dataset[x] for x in tokenizer.model_input_names}
test_set_for_final_model = tf.data.Dataset.from_tensor_slices((test_features, tf_test_dataset["class"]))
test_set_for_final_model =test_set_for_final_model.batch(8)

"""**Defining The Model**

With this line of code, the model would be pooled in to our local machine from huggingface repository
"""

model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=3)

"""**Model Compilation**"""

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=tf.metrics.SparseCategoricalAccuracy(),
)

"""**Training the Model**"""

history = model.fit(train_set_for_final_model, validation_data=val_set_for_final_model, epochs=2 )

"""**Plotting Training and Validation Accuracy**"""

plt.plot(history.history['sparse_categorical_accuracy'])
plt.plot(history.history['val_sparse_categorical_accuracy'])
plt.title('model sparse categorical accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""**Plotting the Training_Loss and Validation_Loss**"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""**Testing the Model with Test Dataset**"""

test_loss, test_acc = model.evaluate(test_set_for_final_model,verbose=2)
print('\nTest accuracy:', test_acc)

"""**Checking the Performance of the Model**"""

predict_score_and_class_dict = {0: 'Hate Speech',
 1: 'Offensive Language',
 2: 'Neither'}

preds = model(tokenizer(["He is such a retard", "That guy is intelligent", "I will kill you" ],return_tensors="tf",padding=True,truncation=True))['logits']

print(preds)

class_preds = np.argmax(preds, axis=1)

for pred in class_preds:
  print(predict_score_and_class_dict[pred])


# Expected Predictions are:
# Hate Speech
# Neither
# Hate Speech





















